{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7567bd-04aa-41de-bd05-e1e97585ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\shaad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f44719-f6ef-45d9-aa61-cb250d89b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set ChromeDriver path\n",
    "chrome_driver_path = r\"C:\\Users\\shaad\\Desktop\\Work\\DS_Assignment\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Read URLs from input.xlsx\n",
    "input_file = \"input.xlsx\"\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create extracted folder if not exists\n",
    "output_folder = \"extracted_script\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Configure Selenium options\n",
    "options = Options()\n",
    "options.headless = True  # Run in headless mode\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# Start the WebDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Iterate through each URL in the Excel file\n",
    "for index, row in df.iterrows():\n",
    "    url_id = str(row[\"URL_ID\"])\n",
    "    url = row[\"URL\"]\n",
    "    \n",
    "    try:\n",
    "        # Open the URL\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load completely\n",
    "\n",
    "        # Extract page source and parse it\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title Found\"\n",
    "\n",
    "        # Extract article content\n",
    "        article_body = soup.find(\"div\", class_=\"td-post-content\")\n",
    "        article_text = article_body.get_text(separator=\"\\n\", strip=True) if article_body else \"No Article Found\"\n",
    "\n",
    "        # Save to text file\n",
    "        file_path = os.path.join(output_folder, f\"{url_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(f\"Title: {title}\\n\\n{article_text}\")\n",
    "        \n",
    "        print(f\"Successfully extracted: {url_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url_id}: {e}\")\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n",
    "print(\"Scraping complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11a61ae-8adb-4e6c-aa86-a8cf1524d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Output saved to Output_nltk.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define paths\n",
    "EXTRACTED_TEXT_FOLDER = \"extracted_script\"\n",
    "STOPWORDS_FOLDER = \"StopWords/StopWords\"\n",
    "MASTER_DICT_FOLDER = \"MasterDictionary/MasterDictionary\"\n",
    "INPUT_FILE = \"Input.xlsx\"\n",
    "OUTPUT_FILE = \"Output_nltk_script.xlsx\"\n",
    "\n",
    "# Load stop words\n",
    "\n",
    "def load_stop_words():\n",
    "    stop_words = set()\n",
    "    for file in os.listdir(STOPWORDS_FOLDER):\n",
    "        with open(os.path.join(STOPWORDS_FOLDER, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            stop_words.update(f.read().split())\n",
    "    return stop_words\n",
    "\n",
    "# Load positive and negative words\n",
    "def load_master_dict():\n",
    "    with open(os.path.join(MASTER_DICT_FOLDER, \"positive-words.txt\"), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        positive_words = set(f.read().split())\n",
    "\n",
    "    with open(os.path.join(MASTER_DICT_FOLDER, \"negative-words.txt\"), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        negative_words = set(f.read().split())\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "\n",
    "stop_words = load_stop_words()\n",
    "positive_words, negative_words = load_master_dict()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Function for sentiment analysis\n",
    "def sentiment_analysis(words):\n",
    "    pos_score = sum(1 for word in words if word in positive_words)\n",
    "    neg_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "    subjectivity_score = (pos_score + neg_score) / (len(words) + 0.000001)\n",
    "    return pos_score, neg_score, polarity_score, subjectivity_score\n",
    "\n",
    "# Function for readability metrics\n",
    "def readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_text(text)\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    complex_words = [word for word in words if syllable_count(word) > 2]\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    perc_complex_words = len(complex_words) / num_words\n",
    "    fog_index = 0.4 * (avg_sentence_length + perc_complex_words)\n",
    "    return avg_sentence_length, perc_complex_words, fog_index, len(complex_words), num_words\n",
    "\n",
    "# Count syllables in a word\n",
    "def syllable_count(word):\n",
    "    return sum(1 for letter in word if letter in \"aeiouAEIOU\")\n",
    "\n",
    "# Count personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len(pronouns)\n",
    "\n",
    "# Compute average word length\n",
    "def avg_word_length(words):\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# Load input file\n",
    "input_df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "# Process each extracted text file\n",
    "results = []\n",
    "for filename in os.listdir(EXTRACTED_TEXT_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_TEXT_FOLDER, filename), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    words = clean_text(text)\n",
    "    pos_score, neg_score, polarity, subjectivity = sentiment_analysis(words)\n",
    "    avg_sent_len, perc_complex, fog_idx, complex_count, word_count = readability_metrics(text)\n",
    "    personal_pronouns = count_personal_pronouns(text)\n",
    "    avg_word_len = avg_word_length(words)\n",
    "    \n",
    "    results.append([\n",
    "        filename, pos_score, neg_score, polarity, subjectivity, avg_sent_len,\n",
    "        perc_complex, fog_idx, avg_sent_len, complex_count, word_count,\n",
    "        avg_word_len, personal_pronouns\n",
    "    ])\n",
    "\n",
    "# Save output to Excel\n",
    "output_df = pd.DataFrame(results, columns=[\n",
    "    \"Filename\", \"Positive Score\", \"Negative Score\", \"Polarity Score\", \"Subjectivity Score\",\n",
    "    \"Avg Sentence Length\", \"Percentage of Complex Words\", \"Fog Index\", \"Avg Words Per Sentence\",\n",
    "    \"Complex Word Count\", \"Word Count\", \"Avg Word Length\", \"Personal Pronouns\"\n",
    "])\n",
    "output_df.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"Analysis completed. Output saved to\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf68ea4-d079-42d3-b827-b59d32d26153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed. Output saved to Output_script.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "EXTRACTED_TEXT_FOLDER = \"extracted_content\"\n",
    "STOPWORDS_FOLDER = \"StopWords\"\n",
    "MASTER_DICT_FOLDER = \"MasterDictionary\"\n",
    "INPUT_FILE = \"Input.xlsx\"\n",
    "OUTPUT_FILE = \"Output_Shaad_Fazal.xlsx\"\n",
    "\n",
    "# Load stop words\n",
    "def load_stop_words():\n",
    "    stop_words = set()\n",
    "    for file in os.listdir(STOPWORDS_FOLDER):\n",
    "        with open(os.path.join(STOPWORDS_FOLDER, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            stop_words.update(f.read().split())\n",
    "    return stop_words\n",
    "\n",
    "# Load positive and negative words\n",
    "def load_master_dict():\n",
    "    with open(os.path.join(MASTER_DICT_FOLDER, \"positive-words.txt\"), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        positive_words = set(f.read().split())\n",
    "\n",
    "    with open(os.path.join(MASTER_DICT_FOLDER, \"negative-words.txt\"), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        negative_words = set(f.read().split())\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "\n",
    "stop_words = load_stop_words()\n",
    "positive_words, negative_words = load_master_dict()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Function for sentiment analysis\n",
    "def sentiment_analysis(words):\n",
    "    pos_score = sum(1 for word in words if word in positive_words)\n",
    "    neg_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
    "    subjectivity_score = (pos_score + neg_score) / (len(words) + 0.000001)\n",
    "    return pos_score, neg_score, polarity_score, subjectivity_score\n",
    "\n",
    "# Function for readability metrics\n",
    "def readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_text(text)\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    complex_words = [word for word in words if syllable_count(word) > 2]\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    perc_complex_words = len(complex_words) / num_words\n",
    "    fog_index = 0.4 * (avg_sentence_length + perc_complex_words)\n",
    "    return avg_sentence_length, perc_complex_words, fog_index, len(complex_words), num_words\n",
    "\n",
    "# Count syllables in a word\n",
    "def syllable_count(word):\n",
    "    return sum(1 for letter in word if letter in \"aeiouAEIOU\")\n",
    "\n",
    "# Count personal pronouns\n",
    "def count_personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
    "    return len(pronouns)\n",
    "\n",
    "# Compute average word length\n",
    "def avg_word_length(words):\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# Load input file\n",
    "input_df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "# Process each extracted text file\n",
    "results = []\n",
    "for index, row in input_df.iterrows():\n",
    "    filename = f\"{row['URL_ID']}.txt\"  # Assuming the URL_ID corresponds to the filename\n",
    "    url = row['URL']  # Get the URL for the current row\n",
    "    file_path = os.path.join(EXTRACTED_TEXT_FOLDER, filename)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        words = clean_text(text)\n",
    "        pos_score, neg_score, polarity, subjectivity = sentiment_analysis(words)\n",
    "        avg_sent_len, perc_complex, fog_idx, complex_count, word_count = readability_metrics(text)\n",
    "        syllable_per_word = sum(syllable_count(word) for word in words) / len(words)  # Calculating syllables per word\n",
    "        personal_pronouns = count_personal_pronouns(text)\n",
    "        avg_word_len = avg_word_length(words)\n",
    "\n",
    "        results.append([\n",
    "            row[\"URL_ID\"], url, pos_score, neg_score, polarity, subjectivity, avg_sent_len,\n",
    "            perc_complex, fog_idx, avg_sent_len, complex_count, word_count,\n",
    "            syllable_per_word, personal_pronouns, avg_word_len\n",
    "        ])\n",
    "\n",
    "# Save output to Excel\n",
    "output_df = pd.DataFrame(results, columns=[\n",
    "    \"URL_ID\", \"URL\", \"Positive Score\", \"Negative Score\", \"Polarity Score\", \"Subjectivity Score\",\n",
    "    \"Avg Sentence Length\", \"Percentage of Complex Words\", \"Fog Index\", \"Avg Number Words Per Sentence\",\n",
    "    \"Complex Word Count\", \"Word Count\", \"Syllable Per Word\", \"Personal Pronouns\", \"Avg Word Length\"\n",
    "])\n",
    "output_df.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(\"Analysis completed. Output saved to\", OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb7158-ef5c-4888-b4a3-be9058986673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datasc)",
   "language": "python",
   "name": "datasc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
